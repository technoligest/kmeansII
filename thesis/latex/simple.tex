

% simple.tex -- Yaser Alkayale Thesis Taken From Vlado Keslj

\documentclass[12pt]{dalthesis}
\usepackage{mathtools}
\usepackage[T1]{fontenc}


% Plots
\usepackage{siunitx}
\usepackage{tikz} % To generate the plot from csv
\usepackage{pgfplots}
\usepgfplotslibrary{external}
\tikzexternalize{simple}
\usepackage{filecontents}
\pgfplotsset{compat=newest} % Allows to place the legend below plot
% \usepgfplotslibrary{units} % Allows to enter the units nicely
\sisetup{
  round-mode          = places,
  round-precision     = 2,
}


% \usepackage{epstopdf} % to turn my eps files into pdf
% \usepackage{tikz}
% \usepgfplotslibrary{external}
% \tikzexternalize{main}


\begin{filecontents*}{table.csv}
  column 1,column 2,c,d
  1,4,5,1
  2,3,1,5
  3,5,6,1
  4,1,4,9
  5,3,4,7
\end{filecontents*}

% These will help for absolute value math operators
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{graphicx}
\graphicspath{ {images/} }

\begin{document}

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother
\newcommand*{\Value}{\frac{1}{2}x^2}


\title{K-means Clustering, An Improved Seeding Process \& Method to Detect Best Number of Clusters }
\author{Yaser Alkayale}


\bcshon  % options are \mcs, \macs, \mec, \mhi, \phd, and \bcshon


\degree{Bachelor's of Computer Science, Honours}
\degreeinitial{B.C.Sc.}
\faculty{Computer Science}
\dept{Faculty of Computer Science}

% Month and Year of Defence
\defencemonth{April}\defenceyear{2018}

\dedicate{This thesis is in dedication to my grandfather whom I was named after.}

\nolistoftables
% \nolistoffigures

\frontmatter

\begin{abstract}
  \paragraph{}
  Clustering is a well-known task that has been studied and used for decades. The idea is to take a set of items and group them into a number of clusters based on a similarity measure. K-means proposed in 1957 by Stuart Lloyd is one of the most widely used clustering algorithm and is still used today for its reasonably fast heuristic to find the clusters based on the Lloyd algorithm and more recent developments in that area. K-means has two main parts to clustering, the initial seeding process and the iteration process. The seeding process picks k initial seeds as cluster centres, and highly affects the accuracy of the final result in the algorithm. The iteration process dominates running time to move the centres around until it converges to an optimum. In this paper, we discuss a new method of the seeding process that gives us more accurate seeds to start the algorithm. We also discuss a novel approach to find an approximation of the correct number of clusters for a given dataset.
\end{abstract}









\begin{acknowledgements} 
  \paragraph{}
  Big thank you to my supervisor Dr. Norbert Zeh. Without his assistance, this project would not have seen the day of light. Thank you to Dr. Vlado Keselj who made himself available when we needed to consult with him. Also big thank you to Arazoo who was with me from the beginning, and went through my ideas with me.
\end{acknowledgements}

\mainmatter


\chapter{Introduction}
\paragraph{}
Clustering problems arise in many domains like
natural language processing\cite{ravichandran2005randomized},
crash report analysis\cite{soto2016machine}, 
and vehicle navigation\cite{maio1996dynamic}.
The notion of what is a good cluster highly depends on the domain and application at hand. Many clustering techniques like hierarchical clustering\cite{corpet1988multiple}
and graph-based techniques\cite{schaeffer2007graph} exist, each serving a different purpose and are practical for different clustering tasks. K-means clustering continues to be one of the most popular clustering algorithms for it's simplicity of implementation and relative efficiency\cite{jain2010data}.
\paragraph{}
Formally, K-means is the problem where we are given a set of n points in d-dimensional space and a number k, where we are asked to split n points into k clusters while minimizing the total sum of distances from points to their cluster centres. K-means does especially well, it terms of speed and accuracy, on convex shaped clusters as it minimizes the sum of distances of all points to their belonging cluster centres. 

\paragraph{}
While K-means is widely used for its proven practical performance, it is not perfect for many use cases. The algorithm can give highly inaccurate clusters if the number k is not known in advance. This is due to the nature of the algorithm forcing all the points into k clusters. Another problem with the algorithm is that it works best on convex shaped clusters, and struggles when there is a lot of noise\footnote{Note: Noise in this context is referring to the overlap of multiple clusters, where it becomes difficult to separate points if they are at the edges of two or more clusters.} in the data. Having noisy data or the incorrect number of clusters k, the K-means algorithm struggles with running time, as it requires a large number of iterations and sometimes never converges to an optimum. 

\paragraph{}
In this paper, we will introduce a new way to seed the K-means algorithm that gives us better initial seeds to help converge the algorithm with fewer iterations and a lower objective function. The seeding process uses multiple runs of the randomized seeding process, and then clusters these to get a more accurate result. 



\chapter{Background}

\section{Seeding Process}
\paragraph{}
The seeding process of the k-means algorithm is crucial to the accuracy of the result because while the iterations converge the centres to an optimum, it is mostly localized to the regions constrained by the initial seeds\cite{arthur2007k}.
Many new approaches have been introduced like K-means++\cite{arthur2007k}, and Kmeans||. It has been experimentally proven that having a better seeding process improves the algorithm by both lowering the objective function of the algorithm, and allowing it to converge faster with less iterations and running time.
\subsection{K-means++}
\paragraph{}
K-means++ uses an intuitive way to seed the initial clusters of the algorithm by trying to pick seeds that are as far apart as possible. This is done by giving a higher probability for points that are further away from the ones already picked. The way it works is that we pick a random initial point, and then given a probability of picking a point proportional to the seeds already picked, until k seeds are picked. 
\paragraph{}
K-means++ works very well because it about evenly distributes the initial seeds in the dataset. Also, because a probablistic model is used, then a higher concentration of points get a high probability of getting at least one point in them.
\subsection{K-means|| (parallel)}
\section{Iterations}
\paragraph{}
The iteration process of the k-means clustering algorithm dominates the running time of the algorithm 

\section{Objective Function}
\paragraph{}
The objective function of the clustering task determines what we are trying to group points based on. In most tasks, including in everything discussed in this paper, the objective function is the sum of squared distances\footnote{Note: Here we used squared distance because we are merely comparing the distances to each other and do not care what the actual values are. Keeping them squared allows us to same on the computational cost of taking the square root of all the distances before summing.}
of all the points to their cluster centres. More formally expressed as:
$$\argmin\limits_r\sum_{i=1}^{k}\sum_{x\in S_i}\norm{ x-u_i}^2$$


\section{Difficulties}
\paragraph{}
K-means clustering is very good at clustering well-separated convex clustered where k the number of clusters in known or accurately measured in advance. However, this is very rarely the case.

\chapter{Analyzing Algorithm Performance}
\section{Normalized Information Distance}
\section{Minimum Weight Perfect Matching on Bipartite Graphs}
\paragraph{}
Perfect matching is the problem where a graph is partitioned into pairs based on a heuristic. Minimum weight perfect matching is where the total costs of the edges connecting the pairs in the graph are minimized. The is an NP-hard problem on a general graph; however, it can be solved in $O(n^3)$. The Hungarian algorithm is one of the best known problems to solve the assignment problem, the more widely used name of the minimum weight perfect matching problem.
\paragraph{}
The assignment problem is where given a set of w workers and t tasks, we are asked to find the
\subsection{Choosing the Correct Matching Function}


\chapter{Experimental Evaluation}
\paragraph
In this section we show how 
\section{Datasets}
\paragraph{}
To test how effective our new techniques are, we 
\subsection{Generated Artificial Datasets}
\paragraph{}
The datasets were generated using 
\subsection{Real World Datasets}


\chapter{Future work}
\section{Moving On}
While k-means is a great clustering method for it's efficiency and simplicity, it is definitely not the greatest algorithm to be used for every single clustering task. 

\chapter{Conclusion}
K-means clustering continues to be one of the most highly used clustering algorithm around the world.



\bibliographystyle{plain}
\bibliography{simple}

\end{document}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Extra latex code that can be reused later.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\textwidth]{image1}
% \caption{Example of a parametric plot }
% \end{figure}



% \begin{figure}[h!]
%   \begin{minipage}{0.48\textwidth}
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\linewidth, % Scale the plot to \linewidth
%         grid=major, % Display a grid
%         grid style={dashed,gray!30}, % Set the style
%         xlabel=X Axis, % Set the labels
%         ylabel=Y Axis,
%         legend style={at={(0.5,-0.3)},anchor=north} % Put the legend below the plot
%         % x tick label style={rotate=90,anchor=east} % Display labels sideways
%       ]
%       \addplot+[mark options={scale=0.1}] table[only marks,col sep=comma] {plotData/dataset0-cluster-0.csv};
%       \addplot+[mark options={scale=0.1}] table[only marks,col sep=comma] {plotData/dataset0-cluster-1.csv};
%       \addplot+[mark options={scale=0.1}] table[only marks,col sep=comma] {plotData/dataset0-cluster-2.csv};
%       \addplot+[mark options={scale=0.1}] table [only marks,col sep=comma] {plotData/dataset0-cluster-3.csv};
%       \legend{Cluster}
%     \end{axis}
%     \end{tikzpicture}
%     \caption{My firsts autogenerated plot.}
%   \end{center}
% \end{minipage}\hfill
% \begin {minipage}{0.48\textwidth}
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\linewidth, % Scale the plot to \linewidth
%         grid=major, % Display a grid
%         grid style={dashed,gray!30}, % Set the style
%         xlabel=X Axis, % Set the labels
%         ylabel=Y Axis,
%         legend style={at={(0.5,-0.3)},anchor=north} % Put the legend below the plot
%         % x tick label style={rotate=90,anchor=east} % Display labels sideways
%       ]
%       \addplot+[mark options={scale=0.1}] table[only marks,col sep=comma] {plotData/dataset1-cluster-0.csv};
%       \addplot+[mark options={scale=0.1}] table[only marks,col sep=comma] {plotData/dataset1-cluster-1.csv};
%       \addplot+[mark options={scale=0.1}] table[only marks,col sep=comma] {plotData/dataset1-cluster-2.csv};
%       \addplot+[mark options={scale=0.1}] table [only marks,col sep=comma] {plotData/dataset1-cluster-3.csv};
%       \legend{Cluster}
%     \end{axis}
%     \end{tikzpicture}
%     \caption{My second autogenerated plot.}
%   \end{center}
% \end{minipage}\hfill
% \end{figure}